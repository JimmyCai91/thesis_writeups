% !TEX root = paper/paper.tex
\vspace{-1.5em}
\section{Introduction}

Anytime recognition is a core competence in human perception, mediating between reflexive recognition and deep analysis of visual input.
Human studies have produced evidence for coarse-to-fine processing of visual input as more time becomes available \cite{Fei-Fei-Vision-2007,mace09plos}.
The underlying mechanisms are unknown, with only a few attempts to explain the temporal dynamics (e.g. via sequential decision processes \cite{hegde08neuro}).
% kadar12vision was cited alongside fei fei

While multi-class recognition in computer vision has achieved levels of performance that allow useful real-world implementation, state-of-the-art methods tend to be computationally expensive and insensitive to Anytime demands.
As these methods are applied at scale, managing their resource consumption (power or cpu-time) cost becomes increasingly important.
For tasks such as personal robotics, the ability to deploy varying levels of processing to different stimuli, depending on computational demands on the robot, also seems crucial.

For most state-of-the-art classification methods, different features are extracted from an image instance at different costs, and contribute differently to decreasing classification error.
Although ``the more features, the better'', high accuracy can be achieved with only a small subset of features for some instances---and different instances benefit from different subsets of features.
For example, simple binary features are sufficient to quickly detect faces \cite{Viola2004} but not more varied visual objects, while the features most useful for separating landscapes from indoor scenes \cite{Xiao-CVPR-2010} are different from those most useful for recognizing fine distinctions between bird species \cite{Farrell-ICCV-2011}.

Computing all features for all images is infeasible in a deployment sensitive to Anytime needs, as each feature brings a significant computational burden.
To deal with this problem, we can set an explicit cost \emph{budget}, specified in terms of wall time or total power expended or another metric.
Additionally, we strive for \emph{Anytime} performance---the ability to terminate the classifier even before the cost budget is depleted and still obtain the best answer.
In this paper, we address the problem of selecting and combining a subset of features under an \emph{Anytime} cost budget.

To exploit the fact that different instances benefit from different subsets of features, our approach to feature selection is a sequential policy.
To learn the policy parameters, we formulate the problem as a Markov Decision Process (MDP) and use reinforcement learning methods.
With different settings of parameters, we can learn policies ranging from \textbf{Static, Myopic}---greedy selection not relying on any observed feature values, to \textbf{Dynamic, Non-myopic}---relying on observed values and considering future actions.

Since test-time efficiency is our motivation, our methods should carry little computational burden.
For this reason, our models are based on linear evaluations, not nearest-neighbor or graphical model methods.
Because different features can be selected for different instances, and because our system may be called upon to give an answer at any point during its execution, the feature combination method needs to be robust to a large number of different observed-feature subsets.
To this end, we present a novel method for learning several classifiers for different clusters of observed-feature subsets.

We evaluate our method on multi-class recognition tasks.
We first demonstrate on synthetic data that our algorithm learns to pick features most useful for the specific test instance.
We demonstrate the advantage of non-myopic over greedy, and of dynamic over static on this and the Scene-15 visual classification dataset.
Then we show results on a subset of the hierarchical ImageNet dataset, where we additionally learn to provide the most specific answers for any desired cost budget and accuracy level.
