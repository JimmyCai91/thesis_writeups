We thank the reviewers for considering our paper in great detail. All three reviewers agree that our method is technically sound, that the paper is well-written, and that the problem is interesting and important.

We agree with R1's helpful suggestion to adjust the rhetoric of our introduction, and to further explicate our distinction from related work. We agree with R2's very thorough and helpful simplification proposals of our claims and notation. Thank you!

R3's main concern is our similarity to the method of Benbouzid 2012 [1]. We believe that this concern is misguided. It is true that they learn a feature-selection policy instead of a static ordering; our text does not at all deny that. However, the space of policies they can learn is severely constrained by the initial ordering of weak classifiers, as their only actions are Continue, Reject, and Skip. The authors explicitly state that their decision to extend the traditional cascade design instead of adopting a fully general action space is guided by the desire to avoid the resulting hard learning problem (second to last paragraph of their related work section). In contrast, we use the fully general action space, and our policies are not restricted.

Regarding R1's discussion of 'unbiased' loss function, we gratefully agree that our statement can be made more precise in exactly the way the reviewer suggests. Another requested clarification: the purpose of the temperature parameter $\tau$ is standard in RL -- the parameter is gradually decreased to control the exploration/exploitation ratio over the course of training. We will adjust existing explanation [394-397] to be explicit about this.

We will also make every attempt to improve clarity of our figures and captions. To address R1's specific concerns: Yes, the right-side plots of Figure 4 are policy trajectories, where more solid lines correspond to more frequently visited sequences of computed features. Figure 5's bottom plots show the fraction of predictions made at inner vs. leaf nodes at different computational budgets. On the left (low budget), inner nodes are shaded because most predictions occur there; on the right (high budget), predictions are more specific and so leaf nodes are now more shaded than the inner nodes.

Regarding R1's suggestion of focusing on real-time applications, we believe that minimizing power consumption for batch processing is equally as important as maximizing real-time speed; in any case, our method is suitable to both: it's a matter only of the cost function and the dataset.

We agree with R3's suggestion for reporting the accuracy vs. cost metric in addition to our "AUC vs. cost" metric. In fact, we have these plots and numbers, and they are what we used to compare to [11] and [24]. We left these plots out solely for space reasons, but will make every effort to include them in the camera ready publication -- and we sincerely apologize for not already including the exact numbers. We further believe that our comparison to these publications is valid, as we use the same (as reported) feature computation code and dataset processing method. Comparing on a ranking task to [24] is quite interesting, but is subject of future publications.

Lastly, R3 requests clarification of our two-stage feature combination method. In this, we are guided by Gehler & Nowozin (CVPR 2009), who show that late-fusion combination of classifiers independently trained on single features performs as well or better than early-fusion combination of feature vectors ("On Feature Combination for Multiclass Object Classification"). We believe late-fusion combination is now standard methodology.

One last thing: as with all publications from our lab, we will release all code of our system and experiments under an open-source license upon publication, with ample documentation. Our code is in Python, and does not rely on any software a non-academic developer cannot freely obtain.
